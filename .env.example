# Coverage Analyzer Configuration
# Copy this file to .env and fill in your API keys

# ============================================
# LLM Provider Configuration
# ============================================

# Choose your LLM provider: "openai", "anthropic", or "ollama"
LLM_PROVIDER=openai

# OpenAI Configuration
OPENAI_API_KEY=sk-your-openai-api-key-here
OPENAI_MODEL=gpt-4-turbo-preview

# Anthropic Configuration (Claude)
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here
ANTHROPIC_MODEL=claude-3-sonnet-20240229

# Ollama Configuration (Local LLM)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama2

# ============================================
# Rate Limiting Configuration
# ============================================

# Maximum requests per minute
RATE_LIMIT_RPM=60

# Maximum tokens per minute
RATE_LIMIT_TPM=90000

# ============================================
# Cache Configuration
# ============================================

# Enable/disable caching (true/false)
CACHE_ENABLED=true

# Cache time-to-live in seconds (default: 1 hour)
CACHE_TTL=3600

# Maximum cache size (number of entries)
CACHE_MAX_SIZE=1000

# ============================================
# Application Settings
# ============================================

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Output format: json, text, markdown
DEFAULT_OUTPUT_FORMAT=json

# Enable verbose mode for detailed output
VERBOSE=false
